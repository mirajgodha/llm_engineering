{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb6c48-df5f-45b0-89c9-4d5b752be7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import random\n",
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel\n",
    "import datetime\n",
    "\n",
    "# -------------------\n",
    "# Hugging Face Login\n",
    "# -------------------\n",
    "login(\"<YOUR_HUGGINGFACE_TOKEN>\")\n",
    "\n",
    "# -------------------\n",
    "# Model Setup\n",
    "# -------------------\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "quant_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quant_config, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "# -------------------\n",
    "# Ask Mistral to Suggest Creative Elements\n",
    "# -------------------\n",
    "def ask_mistral(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(**input_ids, max_new_tokens=200)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "categories = {\n",
    "    \"characters\": \"Give a list of 10 fun and creative cartoon-style characters for kids.\",\n",
    "    \"actions\": \"List 10 imaginative actions a character can do in a kids animation.\",\n",
    "    \"settings\": \"Suggest 10 colorful and playful settings for a kids animation.\",\n",
    "    \"styles\": \"Provide 10 fun visual styles for kids animation (e.g., cartoon, 3D, watercolor).\"\n",
    "}\n",
    "\n",
    "creative_elements = {}\n",
    "for key, prompt in categories.items():\n",
    "    response = ask_mistral(prompt)\n",
    "    creative_elements[key] = [line.strip(\"- \") for line in response.split(\"\\n\") if line.strip() and not line.startswith(\"<\")]\n",
    "\n",
    "# -------------------\n",
    "# Generate Series of Prompts with Same Character\n",
    "# -------------------\n",
    "def generate_prompt_with_character(character, actions, settings, styles):\n",
    "    return f\"{character} {random.choice(actions)} {random.choice(settings)} {random.choice(styles)}\"\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Generate Metadata for YouTube Upload Using Mistral\n",
    "# -------------------\n",
    "metadata_prompt = f\"\"\"Generate YouTube metadata for a kids animation video.\n",
    "Use the following prompts for context: {prompts_list}\n",
    "\n",
    "Format your reply as:\n",
    "Title: <title>\n",
    "Description: <engaging description>\n",
    "Tags: tag1, tag2, tag3, ...\n",
    "\"\"\"\n",
    "metadata_response = ask_mistral(metadata_prompt)\n",
    "\n",
    "# Parse structured metadata\n",
    "lines = metadata_response.split(\"\\n\")\n",
    "title = \"\"\n",
    "description = \"\"\n",
    "tags = []\n",
    "\n",
    "for line in lines:\n",
    "    if line.lower().startswith(\"title:\"):\n",
    "        title = line.split(\":\", 1)[1].strip()\n",
    "    elif line.lower().startswith(\"description:\"):\n",
    "        description = line.split(\":\", 1)[1].strip()\n",
    "    elif line.lower().startswith(\"tags:\"):\n",
    "        tags = [tag.strip() for tag in line.split(\":\", 1)[1].split(\",\")]\n",
    "\n",
    "# -------------------\n",
    "# Final Video Metadata Assignment\n",
    "# -------------------\n",
    "date_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "VIDEO_FILE = f\"output_{date_str}.mp4\"\n",
    "VIDEO_TITLE = title\n",
    "VIDEO_DESCRIPTION = description\n",
    "VIDEO_CATEGORY_ID = \"22\"  # People & Blogs\n",
    "VIDEO_TAGS = tags\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Generate Series of Prompts for < 60 sec Video\n",
    "# -------------------\n",
    "main_character = random.choice(creative_elements[\"characters\"])\n",
    "actions = creative_elements[\"actions\"]\n",
    "settings = creative_elements[\"settings\"]\n",
    "styles = creative_elements[\"styles\"]\n",
    "\n",
    "prompts_list = [generate_prompt_with_character(main_character, actions, settings, styles) for _ in range(5)]\n",
    "selected_character = main_character\n",
    "\n",
    "for i, prompt in enumerate(prompts_list):\n",
    "    print(f\"\\nðŸŽ¬ Prompt {i+1}: {prompt}\")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------\n",
    "# Video Generation Using HunyuanVideoPipeline\n",
    "# -------------------\n",
    "import torch\n",
    "from diffusers import HunyuanVideoPipeline, HunyuanVideoTransformer3DModel, BitsAndBytesConfig\n",
    "model_id = \"hunyuanvideo-community/HunyuanVideo\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "transformer = HunyuanVideoTransformer3DModel.from_pretrained(\n",
    "    model_id,\n",
    "    subfolder=\"transformer\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Initialize pipeline\n",
    "pipe = HunyuanVideoPipeline.from_pretrained(\n",
    "    model_id, \n",
    "    transformer=transformer, \n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "pipe.vae.enable_tiling()        # Memory optimization\n",
    "pipe.enable_model_cpu_offload() # Offload to CPU if needed\n",
    "\n",
    "all_frames = []\n",
    "\n",
    "for i, prompt in enumerate(prompts_list):\n",
    "    print(f\"ðŸŽ¬ Generating segment {i+1}/{len(prompts_list)}: {prompt}\")\n",
    "    \n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        height=320,\n",
    "        width=512,\n",
    "        num_frames=100,          # ~4 sec at 15 fps\n",
    "        num_inference_steps=30\n",
    "    ).frames\n",
    "    \n",
    "    all_frames.append(output)\n",
    "\n",
    "import numpy as np\n",
    "np.save(\"debug_video_frames.npy\", all_frames)\n",
    "print(\"ðŸ§ª Raw frames saved to debug_video_frames.npy\")\n",
    "\n",
    "# -------------------\n",
    "# Save Video to Disk (OpenCV)\n",
    "# -------------------\n",
    "# Flatten double-nested all_frames to a single list of PIL.Image.Imageimport cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "\n",
    "video_path = VIDEO_FILE\n",
    "fps = 15\n",
    "\n",
    "frames = []\n",
    "for segment in all_frames:\n",
    "    for sequence in segment:\n",
    "        for frame in sequence:\n",
    "            if isinstance(frame, Image.Image):\n",
    "                frames.append(frame)\n",
    "\n",
    "if not frames:\n",
    "    raise ValueError(\"No frames found to write into video. Please check generation pipeline.\")\n",
    "\n",
    "\n",
    "width, height = frames[0].size\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n",
    "\n",
    "for frame in frames:\n",
    "    np_frame = np.array(frame)\n",
    "    bgr_frame = cv2.cvtColor(np_frame, cv2.COLOR_RGB2BGR)\n",
    "    video_writer.write(bgr_frame)\n",
    "\n",
    "video_writer.release()\n",
    "print(f\"âœ… Video saved as {video_path}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "llmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
